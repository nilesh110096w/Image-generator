{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tomato_leaf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNRu8om7sCjqSC+uAYUZfjB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilesh110096w/Image-generator/blob/main/tomato_leaf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "xUibTR0qa2c5",
        "outputId": "ef448945-5c87-482e-9877-19b1003fef21"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3043a806-0d37-489c-a86a-c973bc64566e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3043a806-0d37-489c-a86a-c973bc64566e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"nilesh110096w\",\"key\":\"b4ececf94223d0e6a58e84039b9f2695\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qbgzceLgsPl"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fJt3dKSgs6T"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMSqWMCYbDaU"
      },
      "source": [
        "!mkdir -p ~/.kaggle\r\n",
        "!cp kaggle.json ~/.kaggle/\r\n",
        "\r\n",
        "#change the permission\r\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3usRlhbbHgk",
        "outputId": "c530b0bc-b6a2-4225-e2dd-dc7d39e2dc6f"
      },
      "source": [
        "#Download dataset from kaggle using kaggle API\r\n",
        "!kaggle datasets download -d kaustubhb999/tomatoleaf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading tomatoleaf.zip to /content\n",
            " 93% 166M/179M [00:01<00:00, 99.2MB/s]\n",
            "100% 179M/179M [00:01<00:00, 115MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb7uV8G-b5Nf",
        "outputId": "108bf235-eb76-4516-c7d8-bae84b9ea97b"
      },
      "source": [
        "#Extracting zipfile of dataset\r\n",
        "\r\n",
        "from zipfile import ZipFile \r\n",
        "file_name = 'tomatoleaf.zip'\r\n",
        "\r\n",
        "with ZipFile(file_name, 'r') as zip:\r\n",
        "  zip.extractall()\r\n",
        "  print('done')\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrGqCWkPg1mA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "988e0f1d-1bc2-49f0-fb47-bb0756934464"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "        rescale=1./255,\r\n",
        "        shear_range=0.2,\r\n",
        "        zoom_range=0.2,\r\n",
        "        horizontal_flip=True)\r\n",
        "\r\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\r\n",
        "\r\n",
        "training_set = train_datagen.flow_from_directory(\r\n",
        "        'tomato/train',\r\n",
        "        target_size=(128, 128),\r\n",
        "        batch_size=64,\r\n",
        "        class_mode='categorical' )\r\n",
        "label_map = (training_set.class_indices)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clw4iTplrNzV",
        "outputId": "b7c4c64d-9fcf-4f48-fe2c-d8d89c3dee28"
      },
      "source": [
        "import keras\r\n",
        "import numpy\r\n",
        "image_data_set = keras.preprocessing.image_dataset_from_directory(\r\n",
        "    'tomato/train',\r\n",
        "    labels=\"inferred\",\r\n",
        "    label_mode= \"int\",\r\n",
        "    class_names=None,\r\n",
        "    color_mode=\"rgb\",\r\n",
        "    batch_size=32,\r\n",
        "    image_size=(256, 256),\r\n",
        "    shuffle=True,\r\n",
        "    seed=None,\r\n",
        "    validation_split=None,\r\n",
        "    subset=None,\r\n",
        "    interpolation=\"bilinear\",\r\n",
        "    follow_links=False,\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#size of image_data_set is 313 batches(10000 images/32 batches)\r\n",
        "print (len(image_data_set))\r\n",
        "\r\n",
        "# and each batch has got 32 images and corresponding label of each image\r\n",
        "batch_no = 0 #\r\n",
        "\r\n",
        "#img_batches_array = numpy.empty(shape = [256,256,3], dtype = int)\r\n",
        "#img_batched_together = numpy.empty(shape = [256,256,3], dtype = int)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "img_per_batch = []\r\n",
        "img_after_being_batched = []\r\n",
        "for img_batches in image_data_set: #read a batch\r\n",
        "  \r\n",
        "  #print(len(img_batches[0]))  #number of images = 32\r\n",
        "  #print(len(img_batches[1]))  #number of labels = 32 (one label for each image)\r\n",
        "\r\n",
        "  \r\n",
        "  #iterating through images and labels in a batch\r\n",
        "  iter = 0\r\n",
        "\r\n",
        "  for img in img_batches[0]: #read each image and label from batches\r\n",
        "      label = int(img_batches[1][iter])\r\n",
        "      img_array = keras.preprocessing.image.img_to_array(img, \r\n",
        "                                                          data_format=None, \r\n",
        "                                                          dtype=None\r\n",
        "                                                        ) \r\n",
        "                    \r\n",
        "      img_per_batch.append([img_array, label])\r\n",
        "      iter = iter + 1\r\n",
        "\r\n",
        "  #Entire batch will be stored in img_after_being_batched\r\n",
        "  img_after_being_batched.append(img_per_batch)\r\n",
        "\r\n",
        "  #clear the batch img_per_batch\r\n",
        "  img_per_batch = []\r\n",
        "\r\n",
        "\r\n",
        "# img_after_being_batched will store all the 10000 images in batch of 32 where\r\n",
        "#each batch has dimension 256*236*3\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "      \r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 files belonging to 10 classes.\n",
            "313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBzFr--SmbVu",
        "outputId": "0d7be179-a18b-498a-d22b-5eebc155cee1"
      },
      "source": [
        "print(img_after_being_batched[0][0])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[[225., 217., 214.],\n",
            "        [227., 219., 216.],\n",
            "        [228., 220., 217.],\n",
            "        ...,\n",
            "        [224., 216., 213.],\n",
            "        [223., 218., 214.],\n",
            "        [222., 217., 213.]],\n",
            "\n",
            "       [[224., 216., 213.],\n",
            "        [226., 218., 215.],\n",
            "        [229., 221., 218.],\n",
            "        ...,\n",
            "        [224., 216., 213.],\n",
            "        [222., 217., 213.],\n",
            "        [220., 215., 211.]],\n",
            "\n",
            "       [[223., 215., 212.],\n",
            "        [226., 218., 215.],\n",
            "        [229., 221., 218.],\n",
            "        ...,\n",
            "        [224., 216., 213.],\n",
            "        [221., 216., 212.],\n",
            "        [217., 212., 208.]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[167., 154., 146.],\n",
            "        [161., 148., 140.],\n",
            "        [183., 170., 162.],\n",
            "        ...,\n",
            "        [177., 164., 158.],\n",
            "        [174., 161., 155.],\n",
            "        [166., 153., 147.]],\n",
            "\n",
            "       [[167., 154., 146.],\n",
            "        [166., 153., 145.],\n",
            "        [163., 150., 142.],\n",
            "        ...,\n",
            "        [170., 157., 151.],\n",
            "        [173., 160., 154.],\n",
            "        [162., 149., 143.]],\n",
            "\n",
            "       [[165., 152., 144.],\n",
            "        [158., 145., 137.],\n",
            "        [184., 171., 163.],\n",
            "        ...,\n",
            "        [164., 151., 145.],\n",
            "        [173., 160., 154.],\n",
            "        [159., 146., 140.]]], dtype=float32), 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZKh4FzRnrN9",
        "outputId": "58f053db-298a-4e69-8b5a-fc424d0501c7"
      },
      "source": [
        "print(img_array.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 256, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsLZ5Ac6pb2Z",
        "outputId": "198ee4a2-9790-47e9-f375-e0eca4c81732"
      },
      "source": [
        "array = numpy.array([1,2,3,4])\r\n",
        "\r\n",
        "print(array)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e1XHsqa1_x-",
        "outputId": "e1d4311b-c0d8-4ea2-c7ce-a79d9e37daa7"
      },
      "source": [
        "final_batches_of_array = numpy.array([img_after_being_batched])\r\n",
        "#final batch is the numpy array that stores 313 batches(each batch having 32 images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhuV9LGd2ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "4e9e5f44-1c2e-4b01-eed2-6a68ee2d70b4"
      },
      "source": [
        "print(final_batches_of_array[0, 0][0].shape) # to get shape of each image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e8539dc3028c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_batches_of_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to get shape of each image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmsqQQ-sp1o1"
      },
      "source": [
        "#To train nerual network using CNN pre-trained model, inception coding\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpH7XMhgp2RF"
      },
      "source": [
        "!nvidia-smi # to view GPU allocated to my session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJsvxFa9sMJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "outputId": "e71dcfd4-b0f5-4a5d-d188-b33f93586ee1"
      },
      "source": [
        "!pip install tensorflow-gpu #to install GPU to google collab environment"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/cc/a27e73cf8b23f2ce4bdd2b7089a42a7819ce6dd7366dceba406ddc5daa9c/tensorflow_gpu-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3MB)\n",
            "\u001b[K     |████████████████████████████████| 394.3MB 37kB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.19.5)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.36.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.4.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.32.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.4.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (54.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.27.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.2.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQnZ5Bptfifr"
      },
      "source": [
        "#To view the images\r\n",
        "\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "for image_label in img_after_being_batched[0]: #iterate over the images in one of the batch\r\n",
        "    image = numpy.array(image_label[0], dtype=numpy.uint8)\r\n",
        "    label = image_label[1]\r\n",
        "    new_image = Image.fromarray(image)  #we can save this image to disk\r\n",
        "    #display(new_image)  \r\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFmFR1zH_BDO"
      },
      "source": [
        "#creating, training and valiation of model\r\n",
        "\r\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3 #import inception model\r\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "import numpy as np\r\n",
        "from glob import glob\r\n",
        "\r\n",
        "\r\n",
        "#re-size all images to this\r\n",
        "\r\n",
        "IMAGE_SIZE = [256,256,3]\r\n",
        "\r\n",
        "train_path = '/content/tomato/train'\r\n",
        "valid_path = '/content/tomato/val'\r\n",
        "\r\n",
        "\r\n",
        "#Using transfer learning, here we used Inception_v3 model for learning\r\n",
        "\r\n",
        "inception = InceptionV3(input_shape= IMAGE_SIZE, include_top=False, weights='imagenet')\r\n",
        "#here include_top = false means dont copy last layer of the inception network\r\n",
        "\r\n",
        "#Now lets freeze all imported layers of inception model i.e make all he layers untrainable\r\n",
        "\r\n",
        "for layer in inception.layers:\r\n",
        "    layer.trainable = False\r\n",
        "\r\n",
        "#store all the train directory data in a variable folder\r\n",
        "\r\n",
        "folder = glob('/content/tomato/train/*') # len(folder) will store total number of classes\r\n",
        "\r\n",
        "#flatten the last layer of inception model or in other words add a flattening mechanism on top of inception\r\n",
        "#last layer\r\n",
        "\r\n",
        "x = Flatten()(inception.output)\r\n",
        "\r\n",
        "#creating softmax layer(last layer)\r\n",
        "\r\n",
        "prediction = Dense(len(folder), activation='softmax')(x)\r\n",
        "\r\n",
        "#creating the final model\r\n",
        "\r\n",
        "model = Model(inputs=inception.input, outputs=prediction) #inception input shape wll be input shape for the model\r\n",
        "\r\n",
        "\r\n",
        "#model compilation\r\n",
        "model.compile(loss='SparseCategoricalCrossentropy', optimizer = 'adam', metrics = ['accuracy'])\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD6SyaWFBDVq",
        "outputId": "b63ef521-7ea7-46dd-a46d-247ff76a99ed"
      },
      "source": [
        "#model training\r\n",
        "\r\n",
        "#Here we will use generator to generate image batches and feed it for training model\r\n",
        "\r\n",
        "#Image generator that will return a batch of images size 32\r\n",
        "#Problem with such Generator in that it get empties as epoch proceeds and hence repeatition is not possible, thats why go for Keras \r\n",
        "#inbuilt data generators\r\n",
        "image_batch = []\r\n",
        "def generate_images():  #will generate a batch of size 32 in each iteration\r\n",
        "    count_no_of_images = 0\r\n",
        "    image_batch = []\r\n",
        "    label_batch = []\r\n",
        "    total_file_scanned = 0\r\n",
        "    for img_single_batch in img_after_being_batched: #iterate over batches(Total batch 313)\r\n",
        "       for img, label in img_single_batch: #iterate over images inside a batch\r\n",
        "          count_no_of_images = count_no_of_images + 1\r\n",
        "          total_file_scanned = total_file_scanned + 1\r\n",
        "          if count_no_of_images == 16 :  #Batch size returned by generator will be 16 in each iteration\r\n",
        "            count_no_of_images = 0\r\n",
        "            yield numpy.array(image_batch), numpy.array(label_batch)\r\n",
        "            image_batch = []\r\n",
        "            label_batch = []\r\n",
        "        \r\n",
        "          #print(count_no_of_images)\r\n",
        "          image_batch.append(img)\r\n",
        "          label_batch.append(label)\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "#using model.fit_generator()\r\n",
        "\r\n",
        "\"\"\"for img in generate_images():\r\n",
        "    print(img[1])\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "#Steps_per_epoch indicates how many times my training suppose to hit the generator and get the batch_size for one epoch\r\n",
        "\r\n",
        "model.fit_generator(generate_images(), steps_per_epoch = 2, epochs = 1000 )"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 13.2972 - accuracy: 0.8667I fucked up\n",
            "2/2 [==============================] - 0s 80ms/step - loss: 33.4880 - accuracy: 0.8710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 71.4139 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 35.7122 - accuracy: 0.8438\n",
            "Epoch 3/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 38.8073 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 27.0981 - accuracy: 0.8438\n",
            "Epoch 4/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 2.8778 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 3.0181 - accuracy: 0.9375\n",
            "Epoch 5/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 53.3512 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 26.6756 - accuracy: 0.8750\n",
            "Epoch 6/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 2.2958 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 7.4637 - accuracy: 0.9375\n",
            "Epoch 7/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 11.3558 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 5.6779 - accuracy: 0.9688\n",
            "Epoch 8/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 9.5288 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 11.3306 - accuracy: 0.9062\n",
            "Epoch 9/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 4.7655 - accuracy: 0.9688\n",
            "Epoch 10/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 13.0517 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 8.5926 - accuracy: 0.9062\n",
            "Epoch 11/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 15.2868 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 16.3922 - accuracy: 0.9062\n",
            "Epoch 12/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 3.2035 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 5.3520 - accuracy: 0.9062\n",
            "Epoch 13/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 13.3423 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 14.9767 - accuracy: 0.8750\n",
            "Epoch 14/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 5.9578 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 3.0405 - accuracy: 0.9375\n",
            "Epoch 15/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 12.7009 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 15.3288 - accuracy: 0.8438\n",
            "Epoch 16/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 3.0328 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 1.5164 - accuracy: 0.9375\n",
            "Epoch 17/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1552 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 18.1036 - accuracy: 0.9375\n",
            "Epoch 18/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 13.6932 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 6.8466 - accuracy: 0.9062\n",
            "Epoch 19/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 51.0427 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 25.9168 - accuracy: 0.9062\n",
            "Epoch 20/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 54.1456 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 35.0753 - accuracy: 0.7812\n",
            "Epoch 21/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 55.0165 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 34.0846 - accuracy: 0.8438\n",
            "Epoch 22/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 37.0949 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 20.9021 - accuracy: 0.8750\n",
            "Epoch 23/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 5.0190 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 2.5095 - accuracy: 0.9688\n",
            "Epoch 24/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5878 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.2939 - accuracy: 0.9375\n",
            "Epoch 25/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1.6217 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 7.4612 - accuracy: 0.8438\n",
            "Epoch 26/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 18.7541 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 12.9787 - accuracy: 0.9062\n",
            "Epoch 27/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 67.3276 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 44.6882 - accuracy: 0.8438\n",
            "Epoch 28/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 64.0598 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 36.4538 - accuracy: 0.8125\n",
            "Epoch 29/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1.8075 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 25.9037 - accuracy: 0.9062\n",
            "Epoch 30/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 68.5504 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 45.5514 - accuracy: 0.9062\n",
            "Epoch 31/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 2.8168 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 22.1096 - accuracy: 0.8750\n",
            "Epoch 32/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 32.9502 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 41.1895 - accuracy: 0.8438\n",
            "Epoch 33/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 64.3447 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 82.7379 - accuracy: 0.6875\n",
            "Epoch 34/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 61.9290 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 53.0261 - accuracy: 0.8125\n",
            "Epoch 35/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 33.5012 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 28.3954 - accuracy: 0.8125\n",
            "Epoch 36/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 34.9859 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 44.3701 - accuracy: 0.8125\n",
            "Epoch 37/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 39.6585 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 58.9335 - accuracy: 0.7500\n",
            "Epoch 38/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 23.4946 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 40.1747 - accuracy: 0.7188\n",
            "Epoch 39/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 7.1326 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 22.8397 - accuracy: 0.9062\n",
            "Epoch 40/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 12.7027 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 59.0275 - accuracy: 0.8125\n",
            "Epoch 41/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 46.4230 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 44.0469 - accuracy: 0.8438\n",
            "Epoch 42/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 63.9570 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 69.2201 - accuracy: 0.5938\n",
            "Epoch 43/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 11.5549 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 63.7095 - accuracy: 0.7500\n",
            "Epoch 44/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 61.3441 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 37.1232 - accuracy: 0.8438\n",
            "Epoch 45/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 99.0159 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 56.4743 - accuracy: 0.7500\n",
            "Epoch 46/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 79.2808 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 71.6841 - accuracy: 0.7500\n",
            "Epoch 47/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 18.0402 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 10.9783 - accuracy: 0.8750\n",
            "Epoch 48/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 61.4191 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 49.0559 - accuracy: 0.8125\n",
            "Epoch 49/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 81.8572 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 40.9625 - accuracy: 0.8438\n",
            "Epoch 50/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 34.9281 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 61.7596 - accuracy: 0.7812\n",
            "Epoch 51/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 190.4219 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 113.5327 - accuracy: 0.7188\n",
            "Epoch 52/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 105.2862 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 80.2845 - accuracy: 0.8125\n",
            "Epoch 53/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 6.0883 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 64.6893 - accuracy: 0.7500\n",
            "Epoch 54/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 52.8854 - accuracy: 0.5625I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 66.2543 - accuracy: 0.6562\n",
            "Epoch 55/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 75.4451 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 143.1611 - accuracy: 0.6250\n",
            "Epoch 56/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 74.6678 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 65.1908 - accuracy: 0.7500\n",
            "Epoch 57/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 34.2314 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 40.7027 - accuracy: 0.8438\n",
            "Epoch 58/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 107.8672 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 88.8274 - accuracy: 0.6875\n",
            "Epoch 59/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 55.7913 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 32.9537 - accuracy: 0.8125\n",
            "Epoch 60/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 8.1429 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 47.0293 - accuracy: 0.8438\n",
            "Epoch 61/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.9425 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 15.5203 - accuracy: 0.8750\n",
            "Epoch 62/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 67.4979 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 45.0739 - accuracy: 0.8125\n",
            "Epoch 63/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 96.3991 - accuracy: 0.5625I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 60.6830 - accuracy: 0.6562\n",
            "Epoch 64/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1771 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 34.8055 - accuracy: 0.8750\n",
            "Epoch 65/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 23.8745 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 61.3704 - accuracy: 0.7188\n",
            "Epoch 66/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 18.4058 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 22.1097 - accuracy: 0.9062\n",
            "Epoch 67/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 30.3687 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 22.1998 - accuracy: 0.8750\n",
            "Epoch 68/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 13.0057 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 6.5028 - accuracy: 0.9062\n",
            "Epoch 69/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 52.5024 - accuracy: 0.5625I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 36.5507 - accuracy: 0.7500\n",
            "Epoch 70/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 33.7445 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 84.1151 - accuracy: 0.8125\n",
            "Epoch 71/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 97.0197 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 106.4641 - accuracy: 0.6562\n",
            "Epoch 72/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 27.6864 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 51.3413 - accuracy: 0.7812\n",
            "Epoch 73/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 38.9499 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 28.1344 - accuracy: 0.9062\n",
            "Epoch 74/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 38.8495 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 19.4248 - accuracy: 0.9062\n",
            "Epoch 75/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 66.7556 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 54.2763 - accuracy: 0.8438\n",
            "Epoch 76/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 11.5537 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 40.4804 - accuracy: 0.8438\n",
            "Epoch 77/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 49.4833 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 49.5286 - accuracy: 0.7500\n",
            "Epoch 78/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 54.8054 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 37.9980 - accuracy: 0.8125\n",
            "Epoch 79/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 23.8207 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 19.2622 - accuracy: 0.8125\n",
            "Epoch 80/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 36.7658 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 42.1492 - accuracy: 0.8750\n",
            "Epoch 81/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 43.5499 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 70.6355 - accuracy: 0.7500\n",
            "Epoch 82/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 56.9094 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 93.5157 - accuracy: 0.6562\n",
            "Epoch 83/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 24.9847 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 62.6184 - accuracy: 0.7812\n",
            "Epoch 84/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 76.9209 - accuracy: 0.5000I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 76.1085 - accuracy: 0.5938\n",
            "Epoch 85/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 84.7518 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 61.8549 - accuracy: 0.7812\n",
            "Epoch 86/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 170.3278 - accuracy: 0.5625I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 148.2961 - accuracy: 0.6250\n",
            "Epoch 87/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 86.4291 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 61.6357 - accuracy: 0.7188\n",
            "Epoch 88/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 22.4568 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 48.5200 - accuracy: 0.8125\n",
            "Epoch 89/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 154.7926 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 104.7978 - accuracy: 0.7812\n",
            "Epoch 90/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 83.5802 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 100.9134 - accuracy: 0.6250\n",
            "Epoch 91/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 102.1815 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 68.5674 - accuracy: 0.7188\n",
            "Epoch 92/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 28.5415 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 29.2946 - accuracy: 0.8750\n",
            "Epoch 93/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 47.9218 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 33.0250 - accuracy: 0.8125\n",
            "Epoch 94/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 120.7706 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 83.6859 - accuracy: 0.7500\n",
            "Epoch 95/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 81.3365 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 71.8162 - accuracy: 0.6875\n",
            "Epoch 96/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 45.9315 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 68.2775 - accuracy: 0.7500\n",
            "Epoch 97/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 123.7608 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 135.3540 - accuracy: 0.6250\n",
            "Epoch 98/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 162.2775 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 109.0247 - accuracy: 0.7188\n",
            "Epoch 99/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 160.6606 - accuracy: 0.5625I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 122.3840 - accuracy: 0.5938\n",
            "Epoch 100/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 34.8244 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 39.3471 - accuracy: 0.7812\n",
            "Epoch 101/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 97.2636 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 91.4241 - accuracy: 0.6562\n",
            "Epoch 102/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 74.6060 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 50.5492 - accuracy: 0.8125\n",
            "Epoch 103/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 44.6020 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 89.8303 - accuracy: 0.7812\n",
            "Epoch 104/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 63.8249 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 118.2540 - accuracy: 0.6875\n",
            "Epoch 105/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 42.9168 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 57.5761 - accuracy: 0.7500\n",
            "Epoch 106/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 82.0937 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 55.2943 - accuracy: 0.7812\n",
            "Epoch 107/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 201.1996 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 158.3389 - accuracy: 0.7188\n",
            "Epoch 108/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 123.3067 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 126.0201 - accuracy: 0.5312\n",
            "Epoch 109/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 52.1894 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 81.9017 - accuracy: 0.7500\n",
            "Epoch 110/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 11.3430 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 16.2553 - accuracy: 0.8125\n",
            "Epoch 111/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 83.5053 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 80.1479 - accuracy: 0.6875\n",
            "Epoch 112/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 68.3088 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 66.9019 - accuracy: 0.7812\n",
            "Epoch 113/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 35.1393 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 57.7228 - accuracy: 0.7500\n",
            "Epoch 114/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 29.3138 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 35.5519 - accuracy: 0.7500\n",
            "Epoch 115/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 27.8120 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 31.0125 - accuracy: 0.8750\n",
            "Epoch 116/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 16.1863 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 37.6227 - accuracy: 0.7500\n",
            "Epoch 117/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000I fucked up\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 4.0224 - accuracy: 0.9688\n",
            "Epoch 118/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 20.4042 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 14.5727 - accuracy: 0.8750\n",
            "Epoch 119/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 19.9901 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 14.7913 - accuracy: 0.8438\n",
            "Epoch 120/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 91.0711 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 63.8905 - accuracy: 0.7500\n",
            "Epoch 121/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 38.5498 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 24.2192 - accuracy: 0.8750\n",
            "Epoch 122/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 11.5803 - accuracy: 0.9375\n",
            "Epoch 123/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 61.7770 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 66.0077 - accuracy: 0.7188\n",
            "Epoch 124/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 67.8553 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 42.5552 - accuracy: 0.7812\n",
            "Epoch 125/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 17.2931 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 20.4092 - accuracy: 0.8750\n",
            "Epoch 126/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 36.7544 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 69.1699 - accuracy: 0.7500\n",
            "Epoch 127/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 50.4682 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 47.3339 - accuracy: 0.7812\n",
            "Epoch 128/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 125.3487 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 132.5471 - accuracy: 0.5938\n",
            "Epoch 129/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 100.0695 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 62.7447 - accuracy: 0.7812\n",
            "Epoch 130/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 11.8633 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 36.7245 - accuracy: 0.7500\n",
            "Epoch 131/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 16.5018 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 52.3579 - accuracy: 0.8125\n",
            "Epoch 132/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 92.9265 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 58.8842 - accuracy: 0.7812\n",
            "Epoch 133/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 41.7257 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 48.0849 - accuracy: 0.8125\n",
            "Epoch 134/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 12.3455 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 24.3475 - accuracy: 0.8125\n",
            "Epoch 135/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 16.0096 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 52.7297 - accuracy: 0.8125\n",
            "Epoch 136/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 49.7458 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 48.0167 - accuracy: 0.7188\n",
            "Epoch 137/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 73.9688 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 38.9333 - accuracy: 0.8125\n",
            "Epoch 138/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 53.0450 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 28.6476 - accuracy: 0.8125\n",
            "Epoch 139/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 6.5142 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 31.3211 - accuracy: 0.8125\n",
            "Epoch 140/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 73.1852 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 49.5118 - accuracy: 0.7812\n",
            "Epoch 141/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 114.3589 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 96.7694 - accuracy: 0.7500\n",
            "Epoch 142/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 8.7791 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 57.9597 - accuracy: 0.7812\n",
            "Epoch 143/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 17.1750 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 87.7628 - accuracy: 0.7812\n",
            "Epoch 144/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 52.1675 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 43.4233 - accuracy: 0.8438\n",
            "Epoch 145/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 70.6581 - accuracy: 0.5625I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 41.7313 - accuracy: 0.7188\n",
            "Epoch 146/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 127.8808 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 127.9800 - accuracy: 0.7188\n",
            "Epoch 147/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 79.9272 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 71.5283 - accuracy: 0.7500\n",
            "Epoch 148/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 47.9422 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 33.4061 - accuracy: 0.8125\n",
            "Epoch 149/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 37.7256 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 41.0286 - accuracy: 0.8438\n",
            "Epoch 150/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 34.4215 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 29.6141 - accuracy: 0.7812\n",
            "Epoch 151/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 64.4271 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 40.4786 - accuracy: 0.7812\n",
            "Epoch 152/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 7.5060 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 19.6840 - accuracy: 0.8438\n",
            "Epoch 153/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 23.5821 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 26.4577 - accuracy: 0.8125\n",
            "Epoch 154/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 96.6258 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 66.0080 - accuracy: 0.8125\n",
            "Epoch 155/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 68.3552 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 61.6377 - accuracy: 0.7812\n",
            "Epoch 156/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 24.9492 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 40.0076 - accuracy: 0.8438\n",
            "Epoch 157/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 10.3111 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 37.6099 - accuracy: 0.8125\n",
            "Epoch 158/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 8.2322 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 12.3740 - accuracy: 0.8750\n",
            "Epoch 159/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 28.3660 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 18.4256 - accuracy: 0.8750\n",
            "Epoch 160/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 16.2045 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 59.3463 - accuracy: 0.9062\n",
            "Epoch 161/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 23.0711 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 57.0750 - accuracy: 0.7188\n",
            "Epoch 162/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 35.8056 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 18.1711 - accuracy: 0.9062\n",
            "Epoch 163/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 39.1276 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 22.3735 - accuracy: 0.8750\n",
            "Epoch 164/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 49.7352 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 38.6003 - accuracy: 0.8438\n",
            "Epoch 165/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 3.8273 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 28.0511 - accuracy: 0.8125\n",
            "Epoch 166/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 40.6268 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 24.5103 - accuracy: 0.8438\n",
            "Epoch 167/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 64.7856 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 70.8713 - accuracy: 0.6562\n",
            "Epoch 168/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 106.3005 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 98.2268 - accuracy: 0.7500\n",
            "Epoch 169/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 12.9457 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 67.6758 - accuracy: 0.7500\n",
            "Epoch 170/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 60.1357 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 48.3106 - accuracy: 0.8438\n",
            "Epoch 171/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 53.9850 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 64.3396 - accuracy: 0.7812\n",
            "Epoch 172/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 88.8078 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 72.4550 - accuracy: 0.7812\n",
            "Epoch 173/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 26.1327 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 27.0518 - accuracy: 0.8750\n",
            "Epoch 174/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 26.6848 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 44.7384 - accuracy: 0.8125\n",
            "Epoch 175/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 24.9992 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 85.0678 - accuracy: 0.7188\n",
            "Epoch 176/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 46.4714 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 39.2618 - accuracy: 0.8125\n",
            "Epoch 177/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 57.4635 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 35.5357 - accuracy: 0.8438\n",
            "Epoch 178/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 20.7585 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 21.7832 - accuracy: 0.8438\n",
            "Epoch 179/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 67.2712 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 84.5921 - accuracy: 0.7188\n",
            "Epoch 180/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 70.7981 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 79.7201 - accuracy: 0.7812\n",
            "Epoch 181/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 60.3928 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 41.3322 - accuracy: 0.8125\n",
            "Epoch 182/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 86.8429 - accuracy: 0.5625I fucked up\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 77.8840 - accuracy: 0.6562\n",
            "Epoch 183/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 27.4642 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 63.8851 - accuracy: 0.7188\n",
            "Epoch 184/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 53.6341 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 34.5517 - accuracy: 0.7188\n",
            "Epoch 185/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 141.8104 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 81.7356 - accuracy: 0.8125\n",
            "Epoch 186/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 47.7447 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 66.5996 - accuracy: 0.7812\n",
            "Epoch 187/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 13.4325 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 6.7163 - accuracy: 0.9375\n",
            "Epoch 188/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 6.8245 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 17.4560 - accuracy: 0.8750\n",
            "Epoch 189/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 166.9803 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 143.1246 - accuracy: 0.6875\n",
            "Epoch 190/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 14.8326 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 20.2384 - accuracy: 0.8438\n",
            "Epoch 191/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 21.8399 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 43.3530 - accuracy: 0.8125\n",
            "Epoch 192/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 15.6418 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 7.8209 - accuracy: 0.9375\n",
            "Epoch 193/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 11.9490 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 33.2491 - accuracy: 0.8750\n",
            "Epoch 194/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 96.2583 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 48.4307 - accuracy: 0.9062\n",
            "Epoch 195/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 99.9781 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 61.0676 - accuracy: 0.7500\n",
            "Epoch 196/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 81.6935 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 78.2755 - accuracy: 0.7188\n",
            "Epoch 197/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 16.3030 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 29.0268 - accuracy: 0.9375\n",
            "Epoch 198/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 66.9237 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 43.9533 - accuracy: 0.7812\n",
            "Epoch 199/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 31.2592 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 40.5640 - accuracy: 0.7500\n",
            "Epoch 200/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 47.6047 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 35.6795 - accuracy: 0.8438\n",
            "Epoch 201/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 40.6272 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 40.2680 - accuracy: 0.8125\n",
            "Epoch 202/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 35.4705 - accuracy: 0.8438\n",
            "Epoch 203/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 25.8585 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 15.1458 - accuracy: 0.8438\n",
            "Epoch 204/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 12.2471 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 27.6511 - accuracy: 0.8125\n",
            "Epoch 205/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 85.7526 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 88.8297 - accuracy: 0.8125\n",
            "Epoch 206/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 107.6375 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 77.5934 - accuracy: 0.7500\n",
            "Epoch 207/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 78.9517 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 41.6133 - accuracy: 0.7812\n",
            "Epoch 208/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 69.7349 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 107.0960 - accuracy: 0.7812\n",
            "Epoch 209/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000I fucked up\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 48.3436 - accuracy: 0.8750\n",
            "Epoch 210/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 167.9736 - accuracy: 0.5000I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 106.7085 - accuracy: 0.6562\n",
            "Epoch 211/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 24.6035 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 48.0367 - accuracy: 0.8125\n",
            "Epoch 212/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0194 - accuracy: 1.0000I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 38.7753 - accuracy: 0.8438\n",
            "Epoch 213/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 112.7802 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 90.8973 - accuracy: 0.6875\n",
            "Epoch 214/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 67.8308 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 52.8127 - accuracy: 0.8125\n",
            "Epoch 215/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 3.0240e-04 - accuracy: 1.0000I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 5.7071 - accuracy: 0.9688\n",
            "Epoch 216/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 89.9380 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 44.9690 - accuracy: 0.8750\n",
            "Epoch 217/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 5.2822e-06 - accuracy: 1.0000I fucked up\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 47.2066 - accuracy: 0.8750\n",
            "Epoch 218/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 32.4233 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 38.9552 - accuracy: 0.8438\n",
            "Epoch 219/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 17.9236 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 76.7950 - accuracy: 0.8750\n",
            "Epoch 220/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 112.7267 - accuracy: 0.5625I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 70.2660 - accuracy: 0.7500\n",
            "Epoch 221/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 24.7661 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 19.9633 - accuracy: 0.8438\n",
            "Epoch 222/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 2.9082 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 55.4547 - accuracy: 0.7812\n",
            "Epoch 223/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 89.3023 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 71.7091 - accuracy: 0.7812\n",
            "Epoch 224/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 117.3486 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 85.2454 - accuracy: 0.7812\n",
            "Epoch 225/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 61.8083 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 44.0621 - accuracy: 0.7812\n",
            "Epoch 226/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 85.2656 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 86.1281 - accuracy: 0.7188\n",
            "Epoch 227/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 179.1833 - accuracy: 0.5625I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 96.1971 - accuracy: 0.7500\n",
            "Epoch 228/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 73.6648 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 67.9927 - accuracy: 0.7812\n",
            "Epoch 229/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 77.1542 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 43.8052 - accuracy: 0.8125\n",
            "Epoch 230/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 20.6662 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 19.2563 - accuracy: 0.9062\n",
            "Epoch 231/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 60.2442 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 44.3921 - accuracy: 0.9062\n",
            "Epoch 232/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 86.4485 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 53.6952 - accuracy: 0.8750\n",
            "Epoch 233/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 2.1604 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 40.7338 - accuracy: 0.8125\n",
            "Epoch 234/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 43.0630 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 32.8981 - accuracy: 0.9062\n",
            "Epoch 235/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 8.6593 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 8.1271 - accuracy: 0.9062\n",
            "Epoch 236/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 65.0020 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 78.6002 - accuracy: 0.7812\n",
            "Epoch 237/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 98.0824 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 61.9351 - accuracy: 0.7812\n",
            "Epoch 238/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 37.4730 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 21.0514 - accuracy: 0.8750\n",
            "Epoch 239/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 94.4552 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 52.3196 - accuracy: 0.8438\n",
            "Epoch 240/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 60.5184 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 80.9653 - accuracy: 0.7188\n",
            "Epoch 241/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 36.9050 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 23.0609 - accuracy: 0.8438\n",
            "Epoch 242/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 27.0775 - accuracy: 0.8438\n",
            "Epoch 243/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 101.9349 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 56.8730 - accuracy: 0.8125\n",
            "Epoch 244/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 38.6631 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 49.1073 - accuracy: 0.7812\n",
            "Epoch 245/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 11.0163 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 46.6459 - accuracy: 0.8125\n",
            "Epoch 246/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 72.7650 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 52.0781 - accuracy: 0.7500\n",
            "Epoch 247/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 61.1679 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 66.4498 - accuracy: 0.7500\n",
            "Epoch 248/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 26.9591 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 39.4251 - accuracy: 0.8125\n",
            "Epoch 249/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 9.3316 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 41.6168 - accuracy: 0.8750\n",
            "Epoch 250/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 41.0097 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 22.4294 - accuracy: 0.7812\n",
            "Epoch 251/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 89.3126 - accuracy: 0.5000I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 94.3220 - accuracy: 0.5938\n",
            "Epoch 252/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 9.0720 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 6.7274 - accuracy: 0.9375\n",
            "Epoch 253/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0000e+00 - accuracy: 1.0000I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 22.4655 - accuracy: 0.9375\n",
            "Epoch 254/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 16.1704 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 71.1922 - accuracy: 0.7500\n",
            "Epoch 255/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 29.5516 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 53.7504 - accuracy: 0.7812\n",
            "Epoch 256/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 51.0148 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 94.8391 - accuracy: 0.7500\n",
            "Epoch 257/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 84.9192 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 65.0460 - accuracy: 0.7500\n",
            "Epoch 258/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 90.7209 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 90.8635 - accuracy: 0.7188\n",
            "Epoch 259/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 28.7955 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 20.2882 - accuracy: 0.9062\n",
            "Epoch 260/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 68.8991 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 64.7529 - accuracy: 0.7188\n",
            "Epoch 261/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 83.8134 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 62.5896 - accuracy: 0.8125\n",
            "Epoch 262/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 8.1090 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 4.0545 - accuracy: 0.9375\n",
            "Epoch 263/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 66.1481 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 38.8867 - accuracy: 0.8125\n",
            "Epoch 264/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 62.2492 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 48.6636 - accuracy: 0.8125\n",
            "Epoch 265/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 76.2310 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 70.8607 - accuracy: 0.7188\n",
            "Epoch 266/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 80.6313 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 97.4061 - accuracy: 0.6562\n",
            "Epoch 267/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 27.4713 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 56.5355 - accuracy: 0.7500\n",
            "Epoch 268/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 35.5552 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 52.6522 - accuracy: 0.7812\n",
            "Epoch 269/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 53.9113 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 71.6500 - accuracy: 0.6562\n",
            "Epoch 270/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 137.0019 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 73.1360 - accuracy: 0.8438\n",
            "Epoch 271/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 147.0938 - accuracy: 0.5625I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 79.7012 - accuracy: 0.6875\n",
            "Epoch 272/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 22.7803 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 47.0761 - accuracy: 0.7812\n",
            "Epoch 273/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 40.9310 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 68.5959 - accuracy: 0.7812\n",
            "Epoch 274/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 81.4020 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 68.3105 - accuracy: 0.6250\n",
            "Epoch 275/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 53.2117 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 47.7567 - accuracy: 0.8438\n",
            "Epoch 276/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 185.7513 - accuracy: 0.4375I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 179.4337 - accuracy: 0.5312\n",
            "Epoch 277/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 14.9254 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 26.4706 - accuracy: 0.9062\n",
            "Epoch 278/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 7.4439 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 14.9352 - accuracy: 0.9062\n",
            "Epoch 279/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 7.6842 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 4.4643 - accuracy: 0.8750\n",
            "Epoch 280/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 49.8597 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 30.9936 - accuracy: 0.8750\n",
            "Epoch 281/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 132.4346 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 121.3459 - accuracy: 0.7500\n",
            "Epoch 282/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 15.6825 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 7.8413 - accuracy: 0.9688\n",
            "Epoch 283/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 59.0719 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 65.4081 - accuracy: 0.8438\n",
            "Epoch 284/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 19.4652 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 114.7985 - accuracy: 0.7500\n",
            "Epoch 285/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 102.0879 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 122.3505 - accuracy: 0.6875\n",
            "Epoch 286/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 113.6699 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 104.8783 - accuracy: 0.6250\n",
            "Epoch 287/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 44.0040 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 90.6157 - accuracy: 0.6875\n",
            "Epoch 288/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 112.3093 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 123.4613 - accuracy: 0.6562\n",
            "Epoch 289/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 161.2731 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 161.2041 - accuracy: 0.6250\n",
            "Epoch 290/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 88.6898 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 80.0126 - accuracy: 0.6875\n",
            "Epoch 291/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 44.4082 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 28.4613 - accuracy: 0.8750\n",
            "Epoch 292/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 153.7224 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 114.1567 - accuracy: 0.6875\n",
            "Epoch 293/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 118.6368 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 99.4726 - accuracy: 0.7812\n",
            "Epoch 294/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 173.7639 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 113.9423 - accuracy: 0.8125\n",
            "Epoch 295/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 173.9165 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 144.8295 - accuracy: 0.7188\n",
            "Epoch 296/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 229.7888 - accuracy: 0.5625I fucked up\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 137.1753 - accuracy: 0.6250\n",
            "Epoch 297/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 140.4514 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 138.3431 - accuracy: 0.6875\n",
            "Epoch 298/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 4.3396 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 159.4115 - accuracy: 0.6562\n",
            "Epoch 299/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 145.6993 - accuracy: 0.5625I fucked up\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 93.8365 - accuracy: 0.6875\n",
            "Epoch 300/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 70.8976 - accuracy: 0.6875I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 95.5417 - accuracy: 0.6875\n",
            "Epoch 301/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 167.8069 - accuracy: 0.5625I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 138.0978 - accuracy: 0.6250\n",
            "Epoch 302/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 98.6254 - accuracy: 0.6250I fucked up\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 73.6014 - accuracy: 0.6875\n",
            "Epoch 303/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 49.0297 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 30.3571 - accuracy: 0.8438\n",
            "Epoch 304/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 37.0199 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 55.8124 - accuracy: 0.7188\n",
            "Epoch 305/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 34.5781 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 79.3968 - accuracy: 0.8438\n",
            "Epoch 306/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 6.8318 - accuracy: 0.9375I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 74.6621 - accuracy: 0.8438\n",
            "Epoch 307/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 36.1343 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 99.5546 - accuracy: 0.7188\n",
            "Epoch 308/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 59.8834 - accuracy: 0.7500I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 33.4202 - accuracy: 0.8438\n",
            "Epoch 309/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 22.1800 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 12.7146 - accuracy: 0.9062\n",
            "Epoch 310/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 4.8778 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 29.0665 - accuracy: 0.8438\n",
            "Epoch 311/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 22.9228 - accuracy: 0.8125I fucked up\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 47.1216 - accuracy: 0.8125\n",
            "Epoch 312/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 83.9068 - accuracy: 0.8750I fucked up\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 112.5490 - accuracy: 0.6875\n",
            "Epoch 313/1000\n",
            "I fucked up\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 158.5406 - accuracy: 0.6875WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 158.5406 - accuracy: 0.6875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f29e3c3ed10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC3sUMzxmV_z"
      },
      "source": [
        "print(img_after_being_batched[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}